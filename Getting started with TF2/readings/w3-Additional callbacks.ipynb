{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional callbacks\n",
    "\n",
    "In this reading we'll be looking at more of the inbuilt callbacks available in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again be using the sklearn diabetes dataset to demonstrate these callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes_dataset = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the input and target variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = diabetes_dataset['data']\n",
    "targets = diabetes_dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data set into training and test sets\n",
    "\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(data, targets, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also build a simple model to fit to the data with our callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n",
    "    Dense(64,activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)        \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.compile(loss='mse',\n",
    "                optimizer=\"adam\",metrics=[\"mse\",\"mae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now onto the callbacks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate scheduler\n",
    "\n",
    "**Usage:** `tf.keras.callbacks.LearningRateScheduler(schedule, verbose=0)`\n",
    "\n",
    "The learning rate scheduler that we implemented in the previous reading as a custom callback is also available as a built in callback. \n",
    "\n",
    "As in our custom callback, the `LearningRateScheduler` in Keras takes a function `schedule` as an argument. \n",
    "\n",
    "This function `schedule` should take two arguments:\n",
    "* The current epoch (as an integer), and\n",
    "* The current learning rate,\n",
    "\n",
    "and return new learning rate for that epoch. \n",
    "\n",
    "The `LearningRateScheduler` also has an optional `verbose` argument, which prints information about the learning rate if it is set to 1.\n",
    "\n",
    "Let's see a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate schedule function\n",
    "\n",
    "def lr_function(epoch, lr):\n",
    "    if epoch % 2 == 0:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr + epoch/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0020000000474974513.\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0020000000949949026.\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.005000000094994903.\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.009999999888241292.\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.01699999977648258.\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.016999999061226845.\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.025999999061226846.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "history = model.fit(train_data, train_targets, epochs=10,\n",
    "                    callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_function, verbose=1)], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use lambda functions to define your `schedule` given an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.3333333333333333.\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.125.\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.07692307692307693.\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.05555555555555555.\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.043478260869565216.\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.03571428571428571.\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.030303030303030304.\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.02631578947368421.\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.023255813953488372.\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.020833333333333332.\n"
     ]
    }
   ],
   "source": [
    "# Train the model with a difference schedule\n",
    "\n",
    "history = model.fit(train_data, train_targets, epochs=10,\n",
    "                    callbacks=[tf.keras.callbacks.LearningRateScheduler(lambda x:1/(3+5*x), verbose=1)], \n",
    "                    verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV logger\n",
    "**Usage** `tf.keras.callbacks.CSVLogger(filename, separator=',', append=False)`\n",
    "\n",
    "This callback streams the results from each epoch into a CSV file.\n",
    "The first line of the CSV file will be the names of pieces of information recorded on each subsequent line, beginning with the epoch and loss value. The values of metrics at the end of each epoch will also be recorded.\n",
    "\n",
    "The only compulsory argument is the `filename` for the log to be streamed to. This could also be a filepath.\n",
    "\n",
    "You can also specify the `separator` to be used between entries on each line.\n",
    "\n",
    "The `append` argument allows you the option to append your results to an existing file with the same name. This can be particularly useful if you are continuing training.\n",
    "\n",
    "Let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with a CSV logger\n",
    "\n",
    "history = model.fit(train_data, train_targets, epochs=10,\n",
    "                    callbacks=[tf.keras.callbacks.CSVLogger(\"results.csv\")], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the information in the CSV file we have created using `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2786.704406</td>\n",
       "      <td>42.461130</td>\n",
       "      <td>2786.7043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2794.469143</td>\n",
       "      <td>42.529080</td>\n",
       "      <td>2794.4692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2710.982783</td>\n",
       "      <td>42.294490</td>\n",
       "      <td>2710.9830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2735.020572</td>\n",
       "      <td>42.449688</td>\n",
       "      <td>2735.0208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2780.782860</td>\n",
       "      <td>42.773120</td>\n",
       "      <td>2780.7827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2689.212194</td>\n",
       "      <td>41.987038</td>\n",
       "      <td>2689.2122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2738.954822</td>\n",
       "      <td>42.391468</td>\n",
       "      <td>2738.9550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2713.816251</td>\n",
       "      <td>42.066574</td>\n",
       "      <td>2713.8162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2678.429620</td>\n",
       "      <td>41.999020</td>\n",
       "      <td>2678.4294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2688.564665</td>\n",
       "      <td>41.984753</td>\n",
       "      <td>2688.5650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              loss        mae        mse\n",
       "epoch                                   \n",
       "0      2786.704406  42.461130  2786.7043\n",
       "1      2794.469143  42.529080  2794.4692\n",
       "2      2710.982783  42.294490  2710.9830\n",
       "3      2735.020572  42.449688  2735.0208\n",
       "4      2780.782860  42.773120  2780.7827\n",
       "5      2689.212194  41.987038  2689.2122\n",
       "6      2738.954822  42.391468  2738.9550\n",
       "7      2713.816251  42.066574  2713.8162\n",
       "8      2678.429620  41.999020  2678.4294\n",
       "9      2688.564665  41.984753  2688.5650"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(\"results.csv\", index_col='epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda callbacks\n",
    "**Usage** `tf.keras.callbacks.LambdaCallback(\n",
    "        on_epoch_begin=None, on_epoch_end=None, \n",
    "        on_batch_begin=None, on_batch_end=None, \n",
    "        on_train_begin=None, on_train_end=None)`\n",
    "\n",
    "Lambda callbacks are used to quickly define simple custom callbacks with the use of lambda functions.\n",
    "\n",
    "Each of the functions require some positional arguments.\n",
    "* `on_epoch_begin` and `on_epoch_end` expect two arguments: `epoch` and `logs`,\n",
    "* `on_batch_begin` and `on_batch_end` expect two arguments: `batch` and `logs` and\n",
    "* `on_train_begin` and `on_train_end` expect one argument: `logs`.\n",
    "\n",
    "Let's see an example of this in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the epoch number at the beginning of each epoch\n",
    "\n",
    "epoch_callback = tf.keras.callbacks.LambdaCallback(\n",
    "    on_epoch_begin=lambda epoch,logs: print('Starting Epoch {}!'.format(epoch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the loss at the end of each batch\n",
    "\n",
    "batch_loss_callback = tf.keras.callbacks.LambdaCallback(\n",
    "    on_batch_end=lambda batch,logs: print('\\t After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inform that training is finished\n",
    "\n",
    "train_finish_callback = tf.keras.callbacks.LambdaCallback(\n",
    "    on_train_end=lambda logs: print('Training finished!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1!\n",
      "\t After batch 0, the loss is 3171.60.\n",
      "\t After batch 1, the loss is 2339.72.\n",
      "\t After batch 2, the loss is 2486.53.\n",
      "\t After batch 3, the loss is 2514.13.\n",
      "Starting Epoch 2!\n",
      "\t After batch 0, the loss is 2396.46.\n",
      "\t After batch 1, the loss is 2675.58.\n",
      "\t After batch 2, the loss is 2981.78.\n",
      "\t After batch 3, the loss is 2443.35.\n",
      "Starting Epoch 3!\n",
      "\t After batch 0, the loss is 2776.82.\n",
      "\t After batch 1, the loss is 2677.68.\n",
      "\t After batch 2, the loss is 2660.50.\n",
      "\t After batch 3, the loss is 2369.14.\n",
      "Starting Epoch 4!\n",
      "\t After batch 0, the loss is 2583.41.\n",
      "\t After batch 1, the loss is 2757.22.\n",
      "\t After batch 2, the loss is 2351.69.\n",
      "\t After batch 3, the loss is 2809.20.\n",
      "Starting Epoch 5!\n",
      "\t After batch 0, the loss is 2739.05.\n",
      "\t After batch 1, the loss is 2836.88.\n",
      "\t After batch 2, the loss is 2650.22.\n",
      "\t After batch 3, the loss is 2219.57.\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the lambda callbacks\n",
    "\n",
    "history = model.fit(train_data, train_targets, epochs=5, batch_size=100,\n",
    "                    callbacks=[epoch_callback, batch_loss_callback,train_finish_callback], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce learning rate on plateau\n",
    "**Usage** `tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.1, \n",
    "            patience=10, \n",
    "            verbose=0, \n",
    "            mode='auto', \n",
    "            min_delta=0.0001, \n",
    "            cooldown=0, \n",
    "            min_lr=0)`\n",
    "\n",
    "The `ReduceLROnPlateau` callback allows reduction of the learning rate when a metric has stopped improving. \n",
    "The arguments are similar to those used in the `EarlyStopping` callback.\n",
    "* The argument `monitor` is used to specify which metric to base the callback on.\n",
    "* The `factor` is the factor by which the learning rate decreases i.e., new_lr=factor*old_lr.\n",
    "* The `patience` is the number of epochs where there is no improvement on the monitored metric before the learning rate is reduced.\n",
    "* The `verbose` argument will produce progress messages when set to 1.\n",
    "* The `mode` determines whether the learning rate will decrease when the monitored quantity stops increasing (`max`) or decreasing (`min`). The `auto` setting causes the callback to infer the mode from the monitored quantity.\n",
    "* The `min_delta` is the smallest change in the monitored quantity to be deemed an improvement.\n",
    "* The `cooldown` is the number of epochs to wait after the learning rate is changed before the callback resumes normal operation.\n",
    "* The `min_lr` is a lower bound on the learning rate that the callback will produce.\n",
    "\n",
    "Let's examine a final example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1!\n",
      "\t After batch 0, the loss is 1967.35.\n",
      "\t After batch 1, the loss is 2246.33.\n",
      "\t After batch 2, the loss is 2362.39.\n",
      "\t After batch 3, the loss is 2578.12.\n",
      "Starting Epoch 2!\n",
      "\t After batch 0, the loss is 2437.04.\n",
      "\t After batch 1, the loss is 1777.52.\n",
      "\t After batch 2, the loss is 2155.72.\n",
      "\t After batch 3, the loss is 2784.06.\n",
      "Starting Epoch 3!\n",
      "\t After batch 0, the loss is 2412.97.\n",
      "\t After batch 1, the loss is 1962.00.\n",
      "\t After batch 2, the loss is 2106.19.\n",
      "\t After batch 3, the loss is 2657.25.\n",
      "Starting Epoch 4!\n",
      "\t After batch 0, the loss is 2370.46.\n",
      "\t After batch 1, the loss is 2247.63.\n",
      "\t After batch 2, the loss is 2363.08.\n",
      "\t After batch 3, the loss is 2145.77.\n",
      "Starting Epoch 5!\n",
      "\t After batch 0, the loss is 2312.10.\n",
      "\t After batch 1, the loss is 2650.41.\n",
      "\t After batch 2, the loss is 2123.16.\n",
      "\t After batch 3, the loss is 2033.97.\n",
      "Starting Epoch 6!\n",
      "\t After batch 0, the loss is 2194.02.\n",
      "\t After batch 1, the loss is 2353.48.\n",
      "\t After batch 2, the loss is 2675.60.\n",
      "\t After batch 3, the loss is 1909.64.\n",
      "Starting Epoch 7!\n",
      "\t After batch 0, the loss is 2426.46.\n",
      "\t After batch 1, the loss is 1809.77.\n",
      "\t After batch 2, the loss is 2575.17.\n",
      "\t After batch 3, the loss is 2317.82.\n",
      "Starting Epoch 8!\n",
      "\t After batch 0, the loss is 2219.52.\n",
      "\t After batch 1, the loss is 1946.66.\n",
      "\t After batch 2, the loss is 2132.78.\n",
      "\t After batch 3, the loss is 2844.37.\n",
      "Starting Epoch 9!\n",
      "\t After batch 0, the loss is 2183.59.\n",
      "\t After batch 1, the loss is 2109.06.\n",
      "\t After batch 2, the loss is 2303.74.\n",
      "\t After batch 3, the loss is 2544.88.\n",
      "Starting Epoch 10!\n",
      "\t After batch 0, the loss is 2585.36.\n",
      "\t After batch 1, the loss is 1994.40.\n",
      "\t After batch 2, the loss is 2251.50.\n",
      "\t After batch 3, the loss is 2293.27.\n",
      "Starting Epoch 11!\n",
      "\t After batch 0, the loss is 1912.70.\n",
      "\t After batch 1, the loss is 1764.35.\n",
      "\t After batch 2, the loss is 2484.56.\n",
      "\t After batch 3, the loss is 2988.15.\n",
      "Starting Epoch 12!\n",
      "\t After batch 0, the loss is 2233.50.\n",
      "\t After batch 1, the loss is 2262.63.\n",
      "\t After batch 2, the loss is 2062.50.\n",
      "\t After batch 3, the loss is 2584.78.\n",
      "Starting Epoch 13!\n",
      "\t After batch 0, the loss is 2104.46.\n",
      "\t After batch 1, the loss is 2889.36.\n",
      "\t After batch 2, the loss is 2119.69.\n",
      "\t After batch 3, the loss is 2007.23.\n",
      "Starting Epoch 14!\n",
      "\t After batch 0, the loss is 2090.08.\n",
      "\t After batch 1, the loss is 2746.54.\n",
      "\t After batch 2, the loss is 2149.15.\n",
      "\t After batch 3, the loss is 2138.09.\n",
      "Starting Epoch 15!\n",
      "\t After batch 0, the loss is 1652.74.\n",
      "\t After batch 1, the loss is 2552.19.\n",
      "\t After batch 2, the loss is 2579.87.\n",
      "\t After batch 3, the loss is 2348.38.\n",
      "Starting Epoch 16!\n",
      "\t After batch 0, the loss is 2415.12.\n",
      "\t After batch 1, the loss is 2557.20.\n",
      "\t After batch 2, the loss is 2294.59.\n",
      "\t After batch 3, the loss is 1852.76.\n",
      "Starting Epoch 17!\n",
      "\t After batch 0, the loss is 2097.24.\n",
      "\t After batch 1, the loss is 2579.95.\n",
      "\t After batch 2, the loss is 2377.04.\n",
      "\t After batch 3, the loss is 2061.68.\n",
      "Starting Epoch 18!\n",
      "\t After batch 0, the loss is 2549.83.\n",
      "\t After batch 1, the loss is 2140.94.\n",
      "\t After batch 2, the loss is 2496.32.\n",
      "\t After batch 3, the loss is 1924.63.\n",
      "Starting Epoch 19!\n",
      "\t After batch 0, the loss is 1692.21.\n",
      "\t After batch 1, the loss is 2499.48.\n",
      "\t After batch 2, the loss is 2590.14.\n",
      "\t After batch 3, the loss is 2344.01.\n",
      "Starting Epoch 20!\n",
      "\t After batch 0, the loss is 2046.03.\n",
      "\t After batch 1, the loss is 2413.51.\n",
      "\t After batch 2, the loss is 2212.43.\n",
      "\t After batch 3, the loss is 2462.28.\n",
      "Starting Epoch 21!\n",
      "\t After batch 0, the loss is 1944.63.\n",
      "\t After batch 1, the loss is 2078.89.\n",
      "\t After batch 2, the loss is 2539.06.\n",
      "\t After batch 3, the loss is 2566.59.\n",
      "Starting Epoch 22!\n",
      "\t After batch 0, the loss is 1954.89.\n",
      "\t After batch 1, the loss is 2003.35.\n",
      "\t After batch 2, the loss is 2555.54.\n",
      "\t After batch 3, the loss is 2620.06.\n",
      "Starting Epoch 23!\n",
      "\t After batch 0, the loss is 2536.56.\n",
      "\t After batch 1, the loss is 2326.20.\n",
      "\t After batch 2, the loss is 2002.26.\n",
      "\t After batch 3, the loss is 2254.14.\n",
      "Starting Epoch 24!\n",
      "\t After batch 0, the loss is 2268.18.\n",
      "\t After batch 1, the loss is 2414.49.\n",
      "\t After batch 2, the loss is 2088.26.\n",
      "\t After batch 3, the loss is 2352.44.\n",
      "Starting Epoch 25!\n",
      "\t After batch 0, the loss is 1729.19.\n",
      "\t After batch 1, the loss is 2596.77.\n",
      "\t After batch 2, the loss is 2275.83.\n",
      "\t After batch 3, the loss is 2530.93.\n",
      "Starting Epoch 26!\n",
      "\t After batch 0, the loss is 2457.07.\n",
      "\t After batch 1, the loss is 2006.96.\n",
      "\t After batch 2, the loss is 2385.63.\n",
      "\t After batch 3, the loss is 2271.29.\n",
      "Starting Epoch 27!\n",
      "\t After batch 0, the loss is 2612.17.\n",
      "\t After batch 1, the loss is 2202.90.\n",
      "\t After batch 2, the loss is 1940.30.\n",
      "\t After batch 3, the loss is 2367.81.\n",
      "Starting Epoch 28!\n",
      "\t After batch 0, the loss is 2546.75.\n",
      "\t After batch 1, the loss is 2179.71.\n",
      "\t After batch 2, the loss is 2002.20.\n",
      "\t After batch 3, the loss is 2405.64.\n",
      "Starting Epoch 29!\n",
      "\t After batch 0, the loss is 2286.29.\n",
      "\t After batch 1, the loss is 2244.78.\n",
      "\t After batch 2, the loss is 2159.24.\n",
      "\t After batch 3, the loss is 2445.68.\n",
      "Starting Epoch 30!\n",
      "\t After batch 0, the loss is 2545.05.\n",
      "\t After batch 1, the loss is 2233.72.\n",
      "\t After batch 2, the loss is 2317.00.\n",
      "\t After batch 3, the loss is 2013.95.\n",
      "Starting Epoch 31!\n",
      "\t After batch 0, the loss is 2412.65.\n",
      "\t After batch 1, the loss is 2370.14.\n",
      "\t After batch 2, the loss is 2271.55.\n",
      "\t After batch 3, the loss is 2061.65.\n",
      "Starting Epoch 32!\n",
      "\t After batch 0, the loss is 2432.93.\n",
      "\t After batch 1, the loss is 1475.41.\n",
      "\t After batch 2, the loss is 2632.52.\n",
      "\t After batch 3, the loss is 2591.57.\n",
      "Starting Epoch 33!\n",
      "\t After batch 0, the loss is 2115.79.\n",
      "\t After batch 1, the loss is 2085.78.\n",
      "\t After batch 2, the loss is 2288.76.\n",
      "\t After batch 3, the loss is 2635.87.\n",
      "Starting Epoch 34!\n",
      "\t After batch 0, the loss is 2376.93.\n",
      "\t After batch 1, the loss is 2351.36.\n",
      "\t After batch 2, the loss is 1975.01.\n",
      "\t After batch 3, the loss is 2421.93.\n",
      "Starting Epoch 35!\n",
      "\t After batch 0, the loss is 2449.09.\n",
      "\t After batch 1, the loss is 2310.33.\n",
      "\t After batch 2, the loss is 2133.92.\n",
      "\t After batch 3, the loss is 2238.44.\n",
      "Starting Epoch 36!\n",
      "\t After batch 0, the loss is 2223.80.\n",
      "\t After batch 1, the loss is 1997.73.\n",
      "\t After batch 2, the loss is 2132.35.\n",
      "\t After batch 3, the loss is 2778.07.\n",
      "Starting Epoch 37!\n",
      "\t After batch 0, the loss is 2782.24.\n",
      "\t After batch 1, the loss is 2125.22.\n",
      "\t After batch 2, the loss is 1615.35.\n",
      "\t After batch 3, the loss is 2612.64.\n",
      "Starting Epoch 38!\n",
      "\t After batch 0, the loss is 2451.58.\n",
      "\t After batch 1, the loss is 2038.68.\n",
      "\t After batch 2, the loss is 2223.15.\n",
      "\t After batch 3, the loss is 2410.63.\n",
      "Starting Epoch 39!\n",
      "\t After batch 0, the loss is 2579.79.\n",
      "\t After batch 1, the loss is 1726.43.\n",
      "\t After batch 2, the loss is 2379.08.\n",
      "\t After batch 3, the loss is 2432.89.\n",
      "Starting Epoch 40!\n",
      "\t After batch 0, the loss is 2544.23.\n",
      "\t After batch 1, the loss is 2147.83.\n",
      "\t After batch 2, the loss is 2074.36.\n",
      "\t After batch 3, the loss is 2352.14.\n",
      "Starting Epoch 41!\n",
      "\t After batch 0, the loss is 2372.34.\n",
      "\t After batch 1, the loss is 1978.03.\n",
      "\t After batch 2, the loss is 2334.59.\n",
      "\t After batch 3, the loss is 2434.75.\n",
      "Starting Epoch 42!\n",
      "\t After batch 0, the loss is 2865.81.\n",
      "\t After batch 1, the loss is 2144.63.\n",
      "\t After batch 2, the loss is 2014.43.\n",
      "\t After batch 3, the loss is 2095.53.\n",
      "Starting Epoch 43!\n",
      "\t After batch 0, the loss is 1952.47.\n",
      "\t After batch 1, the loss is 2675.82.\n",
      "\t After batch 2, the loss is 2427.75.\n",
      "\t After batch 3, the loss is 2054.04.\n",
      "Starting Epoch 44!\n",
      "\t After batch 0, the loss is 2356.47.\n",
      "\t After batch 1, the loss is 1919.69.\n",
      "\t After batch 2, the loss is 2322.71.\n",
      "\t After batch 3, the loss is 2518.41.\n",
      "Starting Epoch 45!\n",
      "\t After batch 0, the loss is 2141.41.\n",
      "\t After batch 1, the loss is 2140.95.\n",
      "\t After batch 2, the loss is 2536.63.\n",
      "\t After batch 3, the loss is 2297.83.\n",
      "Starting Epoch 46!\n",
      "\t After batch 0, the loss is 1714.87.\n",
      "\t After batch 1, the loss is 2273.56.\n",
      "\t After batch 2, the loss is 2867.37.\n",
      "\t After batch 3, the loss is 2258.76.\n",
      "Starting Epoch 47!\n",
      "\t After batch 0, the loss is 1765.42.\n",
      "\t After batch 1, the loss is 2778.59.\n",
      "\t After batch 2, the loss is 2613.94.\n",
      "\t After batch 3, the loss is 1961.15.\n",
      "Starting Epoch 48!\n",
      "\t After batch 0, the loss is 2545.89.\n",
      "\t After batch 1, the loss is 2162.19.\n",
      "\t After batch 2, the loss is 2217.15.\n",
      "\t After batch 3, the loss is 2188.37.\n",
      "Starting Epoch 49!\n",
      "\t After batch 0, the loss is 2451.53.\n",
      "\t After batch 1, the loss is 2243.59.\n",
      "\t After batch 2, the loss is 2310.07.\n",
      "\t After batch 3, the loss is 2099.32.\n",
      "Starting Epoch 50!\n",
      "\t After batch 0, the loss is 2322.58.\n",
      "\t After batch 1, the loss is 2077.97.\n",
      "\t After batch 2, the loss is 2457.00.\n",
      "\t After batch 3, the loss is 2249.44.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 51!\n",
      "\t After batch 0, the loss is 2071.55.\n",
      "\t After batch 1, the loss is 2223.31.\n",
      "\t After batch 2, the loss is 2257.74.\n",
      "\t After batch 3, the loss is 2565.78.\n",
      "Starting Epoch 52!\n",
      "\t After batch 0, the loss is 2440.78.\n",
      "\t After batch 1, the loss is 2587.35.\n",
      "\t After batch 2, the loss is 2203.16.\n",
      "\t After batch 3, the loss is 1872.25.\n",
      "Starting Epoch 53!\n",
      "\t After batch 0, the loss is 2043.65.\n",
      "\t After batch 1, the loss is 2628.90.\n",
      "\t After batch 2, the loss is 2274.61.\n",
      "\t After batch 3, the loss is 2167.17.\n",
      "Starting Epoch 54!\n",
      "\t After batch 0, the loss is 1902.84.\n",
      "\t After batch 1, the loss is 2281.98.\n",
      "\t After batch 2, the loss is 2406.54.\n",
      "\t After batch 3, the loss is 2524.76.\n",
      "Starting Epoch 55!\n",
      "\t After batch 0, the loss is 2395.45.\n",
      "\t After batch 1, the loss is 2008.91.\n",
      "\t After batch 2, the loss is 2642.49.\n",
      "\t After batch 3, the loss is 2070.65.\n",
      "Starting Epoch 56!\n",
      "\t After batch 0, the loss is 2207.88.\n",
      "\t After batch 1, the loss is 2608.59.\n",
      "\t After batch 2, the loss is 2167.97.\n",
      "\t After batch 3, the loss is 2123.19.\n",
      "Starting Epoch 57!\n",
      "\t After batch 0, the loss is 2207.43.\n",
      "\t After batch 1, the loss is 1970.44.\n",
      "\t After batch 2, the loss is 1973.05.\n",
      "\t After batch 3, the loss is 2978.83.\n",
      "Starting Epoch 58!\n",
      "\t After batch 0, the loss is 2541.46.\n",
      "\t After batch 1, the loss is 2154.01.\n",
      "\t After batch 2, the loss is 2474.84.\n",
      "\t After batch 3, the loss is 1965.02.\n",
      "Starting Epoch 59!\n",
      "\t After batch 0, the loss is 2637.70.\n",
      "\t After batch 1, the loss is 2324.19.\n",
      "\t After batch 2, the loss is 2273.25.\n",
      "\t After batch 3, the loss is 1868.54.\n",
      "Starting Epoch 60!\n",
      "\t After batch 0, the loss is 2044.13.\n",
      "\t After batch 1, the loss is 2095.65.\n",
      "\t After batch 2, the loss is 2222.71.\n",
      "\t After batch 3, the loss is 2762.53.\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.0008333333767950535.\n",
      "Starting Epoch 61!\n",
      "\t After batch 0, the loss is 2231.15.\n",
      "\t After batch 1, the loss is 2471.37.\n",
      "\t After batch 2, the loss is 2312.56.\n",
      "\t After batch 3, the loss is 2085.59.\n",
      "Starting Epoch 62!\n",
      "\t After batch 0, the loss is 2523.18.\n",
      "\t After batch 1, the loss is 2278.29.\n",
      "\t After batch 2, the loss is 2555.11.\n",
      "\t After batch 3, the loss is 1732.82.\n",
      "Starting Epoch 63!\n",
      "\t After batch 0, the loss is 2204.38.\n",
      "\t After batch 1, the loss is 1961.68.\n",
      "\t After batch 2, the loss is 2592.96.\n",
      "\t After batch 3, the loss is 2350.04.\n",
      "Starting Epoch 64!\n",
      "\t After batch 0, the loss is 2681.71.\n",
      "\t After batch 1, the loss is 2314.20.\n",
      "\t After batch 2, the loss is 1834.69.\n",
      "\t After batch 3, the loss is 2277.29.\n",
      "Starting Epoch 65!\n",
      "\t After batch 0, the loss is 2655.45.\n",
      "\t After batch 1, the loss is 2070.04.\n",
      "\t After batch 2, the loss is 2585.71.\n",
      "\t After batch 3, the loss is 1775.92.\n",
      "Starting Epoch 66!\n",
      "\t After batch 0, the loss is 2489.80.\n",
      "\t After batch 1, the loss is 1737.54.\n",
      "\t After batch 2, the loss is 2204.40.\n",
      "\t After batch 3, the loss is 2683.98.\n",
      "Starting Epoch 67!\n",
      "\t After batch 0, the loss is 2379.41.\n",
      "\t After batch 1, the loss is 2490.11.\n",
      "\t After batch 2, the loss is 2050.80.\n",
      "\t After batch 3, the loss is 2178.77.\n",
      "Starting Epoch 68!\n",
      "\t After batch 0, the loss is 2038.54.\n",
      "\t After batch 1, the loss is 2227.34.\n",
      "\t After batch 2, the loss is 2366.57.\n",
      "\t After batch 3, the loss is 2474.76.\n",
      "Starting Epoch 69!\n",
      "\t After batch 0, the loss is 2233.82.\n",
      "\t After batch 1, the loss is 2406.78.\n",
      "\t After batch 2, the loss is 2251.01.\n",
      "\t After batch 3, the loss is 2207.60.\n",
      "Starting Epoch 70!\n",
      "\t After batch 0, the loss is 2835.34.\n",
      "\t After batch 1, the loss is 2001.72.\n",
      "\t After batch 2, the loss is 2052.36.\n",
      "\t After batch 3, the loss is 2210.51.\n",
      "Starting Epoch 71!\n",
      "\t After batch 0, the loss is 2777.72.\n",
      "\t After batch 1, the loss is 2118.51.\n",
      "\t After batch 2, the loss is 1811.35.\n",
      "\t After batch 3, the loss is 2396.89.\n",
      "Starting Epoch 72!\n",
      "\t After batch 0, the loss is 2548.90.\n",
      "\t After batch 1, the loss is 2250.40.\n",
      "\t After batch 2, the loss is 2579.53.\n",
      "\t After batch 3, the loss is 1706.53.\n",
      "Starting Epoch 73!\n",
      "\t After batch 0, the loss is 2710.33.\n",
      "\t After batch 1, the loss is 1946.07.\n",
      "\t After batch 2, the loss is 2343.41.\n",
      "\t After batch 3, the loss is 2095.90.\n",
      "Starting Epoch 74!\n",
      "\t After batch 0, the loss is 2082.28.\n",
      "\t After batch 1, the loss is 2326.60.\n",
      "\t After batch 2, the loss is 2276.19.\n",
      "\t After batch 3, the loss is 2420.31.\n",
      "Starting Epoch 75!\n",
      "\t After batch 0, the loss is 2290.38.\n",
      "\t After batch 1, the loss is 1997.43.\n",
      "\t After batch 2, the loss is 2221.57.\n",
      "\t After batch 3, the loss is 2600.90.\n",
      "Starting Epoch 76!\n",
      "\t After batch 0, the loss is 2656.86.\n",
      "\t After batch 1, the loss is 1995.93.\n",
      "\t After batch 2, the loss is 2103.59.\n",
      "\t After batch 3, the loss is 2347.44.\n",
      "Starting Epoch 77!\n",
      "\t After batch 0, the loss is 2414.54.\n",
      "\t After batch 1, the loss is 2107.50.\n",
      "\t After batch 2, the loss is 2103.48.\n",
      "\t After batch 3, the loss is 2480.69.\n",
      "Starting Epoch 78!\n",
      "\t After batch 0, the loss is 1841.16.\n",
      "\t After batch 1, the loss is 2383.27.\n",
      "\t After batch 2, the loss is 2199.52.\n",
      "\t After batch 3, the loss is 2689.49.\n",
      "Starting Epoch 79!\n",
      "\t After batch 0, the loss is 1890.83.\n",
      "\t After batch 1, the loss is 2450.48.\n",
      "\t After batch 2, the loss is 2843.12.\n",
      "\t After batch 3, the loss is 1905.32.\n",
      "Starting Epoch 80!\n",
      "\t After batch 0, the loss is 1883.40.\n",
      "\t After batch 1, the loss is 2673.20.\n",
      "\t After batch 2, the loss is 2042.81.\n",
      "\t After batch 3, the loss is 2507.69.\n",
      "Starting Epoch 81!\n",
      "\t After batch 0, the loss is 2118.86.\n",
      "\t After batch 1, the loss is 2425.04.\n",
      "\t After batch 2, the loss is 2209.35.\n",
      "\t After batch 3, the loss is 2349.43.\n",
      "Starting Epoch 82!\n",
      "\t After batch 0, the loss is 2060.09.\n",
      "\t After batch 1, the loss is 2612.19.\n",
      "\t After batch 2, the loss is 2407.25.\n",
      "\t After batch 3, the loss is 2014.97.\n",
      "Starting Epoch 83!\n",
      "\t After batch 0, the loss is 2539.21.\n",
      "\t After batch 1, the loss is 2561.91.\n",
      "\t After batch 2, the loss is 1999.17.\n",
      "\t After batch 3, the loss is 1990.77.\n",
      "Starting Epoch 84!\n",
      "\t After batch 0, the loss is 2228.80.\n",
      "\t After batch 1, the loss is 2864.42.\n",
      "\t After batch 2, the loss is 2428.45.\n",
      "\t After batch 3, the loss is 1556.36.\n",
      "Starting Epoch 85!\n",
      "\t After batch 0, the loss is 2553.65.\n",
      "\t After batch 1, the loss is 1994.64.\n",
      "\t After batch 2, the loss is 2593.62.\n",
      "\t After batch 3, the loss is 1950.23.\n",
      "Starting Epoch 86!\n",
      "\t After batch 0, the loss is 2426.22.\n",
      "\t After batch 1, the loss is 2573.89.\n",
      "\t After batch 2, the loss is 1564.83.\n",
      "\t After batch 3, the loss is 2542.82.\n",
      "Starting Epoch 87!\n",
      "\t After batch 0, the loss is 2432.18.\n",
      "\t After batch 1, the loss is 2253.62.\n",
      "\t After batch 2, the loss is 2355.46.\n",
      "\t After batch 3, the loss is 2053.47.\n",
      "Starting Epoch 88!\n",
      "\t After batch 0, the loss is 2481.46.\n",
      "\t After batch 1, the loss is 2235.20.\n",
      "\t After batch 2, the loss is 2302.01.\n",
      "\t After batch 3, the loss is 2075.55.\n",
      "Starting Epoch 89!\n",
      "\t After batch 0, the loss is 2552.93.\n",
      "\t After batch 1, the loss is 2484.92.\n",
      "\t After batch 2, the loss is 2001.98.\n",
      "\t After batch 3, the loss is 2053.24.\n",
      "Starting Epoch 90!\n",
      "\t After batch 0, the loss is 2261.59.\n",
      "\t After batch 1, the loss is 2264.32.\n",
      "\t After batch 2, the loss is 2403.88.\n",
      "\t After batch 3, the loss is 2167.00.\n",
      "Starting Epoch 91!\n",
      "\t After batch 0, the loss is 2112.66.\n",
      "\t After batch 1, the loss is 3041.48.\n",
      "\t After batch 2, the loss is 2162.13.\n",
      "\t After batch 3, the loss is 1769.79.\n",
      "Starting Epoch 92!\n",
      "\t After batch 0, the loss is 2108.66.\n",
      "\t After batch 1, the loss is 2780.23.\n",
      "\t After batch 2, the loss is 1630.11.\n",
      "\t After batch 3, the loss is 2589.76.\n",
      "Starting Epoch 93!\n",
      "\t After batch 0, the loss is 2212.88.\n",
      "\t After batch 1, the loss is 1828.73.\n",
      "\t After batch 2, the loss is 2208.94.\n",
      "\t After batch 3, the loss is 2866.76.\n",
      "Starting Epoch 94!\n",
      "\t After batch 0, the loss is 2118.86.\n",
      "\t After batch 1, the loss is 1847.24.\n",
      "\t After batch 2, the loss is 2953.65.\n",
      "\t After batch 3, the loss is 2177.27.\n",
      "Starting Epoch 95!\n",
      "\t After batch 0, the loss is 1762.71.\n",
      "\t After batch 1, the loss is 2283.51.\n",
      "\t After batch 2, the loss is 2627.96.\n",
      "\t After batch 3, the loss is 2430.01.\n",
      "Starting Epoch 96!\n",
      "\t After batch 0, the loss is 2388.22.\n",
      "\t After batch 1, the loss is 1947.41.\n",
      "\t After batch 2, the loss is 2467.34.\n",
      "\t After batch 3, the loss is 2296.59.\n",
      "Starting Epoch 97!\n",
      "\t After batch 0, the loss is 2256.51.\n",
      "\t After batch 1, the loss is 2370.89.\n",
      "\t After batch 2, the loss is 2177.85.\n",
      "\t After batch 3, the loss is 2295.94.\n",
      "Starting Epoch 98!\n",
      "\t After batch 0, the loss is 2204.99.\n",
      "\t After batch 1, the loss is 2155.85.\n",
      "\t After batch 2, the loss is 2654.47.\n",
      "\t After batch 3, the loss is 2078.19.\n",
      "Starting Epoch 99!\n",
      "\t After batch 0, the loss is 2210.92.\n",
      "\t After batch 1, the loss is 1961.18.\n",
      "\t After batch 2, the loss is 2242.94.\n",
      "\t After batch 3, the loss is 2695.83.\n",
      "Starting Epoch 100!\n",
      "\t After batch 0, the loss is 2503.10.\n",
      "\t After batch 1, the loss is 2225.53.\n",
      "\t After batch 2, the loss is 2268.92.\n",
      "\t After batch 3, the loss is 2095.78.\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the ReduceLROnPlateau callback\n",
    "\n",
    "history = model.fit(train_data, train_targets, epochs=100, batch_size=100,\n",
    "                    callbacks=[\n",
    "                        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\",factor=0.2, verbose=1),\n",
    "                        epoch_callback, \n",
    "                        batch_loss_callback, \n",
    "                        train_finish_callback\n",
    "                    ], \n",
    "                    verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading and resources\n",
    "* https://keras.io/callbacks/\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/CSVLogger\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LambdaCallback"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
